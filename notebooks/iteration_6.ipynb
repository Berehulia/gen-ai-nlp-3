{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtcRLw8iYbHn",
    "outputId": "a0b4751c-3d25-4b05-bc83-0e9922bbac57"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers bitsandbytes langchain langchain-community langchain-huggingface langchain-text-splitters pymupdf faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVX8NBrGYbHp"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAElqhptYbHq"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "TRAIN_FILE = \"zno.train.jsonl\"\n",
    "TEST_FILE = \"zno.test.jsonl\"\n",
    "OUTPUT_FILE = \"submission.csv\"\n",
    "DEBUG_LIMIT = 100\n",
    "\n",
    "DATA_FOLDER = \"ubertext_data\"\n",
    "WIKIPEDIA_URL = \"https://lang.org.ua/static/downloads/ubertext2.0/wikipedia/cleansed/ubertext.wikipedia.filter_rus_gcld+short.text_only.txt.bz2\"\n",
    "INDEX_PATH = \"faiss_index\"\n",
    "CHUNKS_CACHE = \"chunks_cache.pkl\"\n",
    "EMBEDDING_MODEL = \"intfloat/multilingual-e5-small\"\n",
    "TOP_K = 3\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -qq lbzip2\n",
    "!mkdir -p {DATA_FOLDER}\n",
    "!wget -nc -P {DATA_FOLDER} {WIKIPEDIA_URL}\n",
    "!cd {DATA_FOLDER} && lbzip2 -dk *.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "d8f699bd0acd406bac1f22e4b6636ea3",
      "f64b80864ad24fe6ba38e0c67a704b3e",
      "1d04f042a942445ba049cd711bad66b2",
      "49590f8038e74a38ace6a581ced9dda5",
      "d14c739e7a534798bcd8136393da2871",
      "85903fe3a20a4a8a9c60a7a52830de19",
      "89cab6a98c4845af984399f7d7e1fa04",
      "7446547f30344d879c0b8480b0bf8b0b",
      "b3f2298531a942e0973cf7c10157d585",
      "2a5f7126c3534a32be21c7b9539be4d1",
      "e98f6fc2d5a146b095503d54eb4960fb",
      "0e734fe151394215a0c6e686f779c656",
      "3432e96fcf584f8993916ff80584b7e7",
      "baa3a6e01389407f8b89b97621ac0015",
      "ccf70f44c7d54b33b90ef3bb59b368da",
      "fce8bf48867d4dcbbaaa45aec8a6b48d",
      "fe5eb988c883446081fd7e154cde4a96",
      "50c6f64abbb94992b45d92f52e594351",
      "09760b824cd34a0b99f459af9ddeaa0b",
      "87b597d80a3642099393556ad0f2e493",
      "e0a34a44eea84c5daaaa4c6fd4e360a1",
      "44ff1ff7f5fc4580a95a85d893223529",
      "a117b5f78f2a43b58b1aede508c16ec2",
      "4b422170a73d4d318744a52685bfe042",
      "047f8c4d38c243d285d08d08d0cc9ebb",
      "bce7020b66e3404b837675f49d497d93",
      "12e3cd787a114efda4bbc783ab6f6576",
      "10426e6e9b1d46aa93befdbe7381ab5c",
      "235df47575b8422b8f01a88293d2a669",
      "f7f3db858691412a9071ea58d669bb0d",
      "3784789d23ac45a6a431266da5487a34",
      "6f2db4529594429590b2beb5163f83cc",
      "ef2fb2e8fd01471cb829efbdd1fc2537",
      "b888b6d072904ab59d13995846622338",
      "5c11aa182b51407399d42f1cefb34604",
      "6fe87bf2d00140d9b6a1a1b82e050920",
      "e925f78c4fcf4d15b7db0e1af0843aa2",
      "8fac0c8e32864bd98a4a609331c7ee55",
      "d7ff5e74bcb6400586ff424ed9920c5f",
      "a6cb003706ec4f78b7ca1587ec677f0a",
      "0404795a0c79472aa8d74eccc14051ff",
      "c24c129e392c4750b7dc20dc01302380",
      "019fbccc01f14088ab042d629e783763",
      "7c0b5ba68de444978a368f502e9cb817",
      "5bc47d87db9b4cc6932680b7092b7ace",
      "d7757850964e4cbd8f12377bf51fdc5b",
      "e464b2494fc14c3c9d0eefcae499230a",
      "e3cec2c8b88347ad92b5df2854704b7b",
      "b601632a425949bf90a0bd12ee7d4306",
      "f16cb78c69b44dfab3bfd35356efb77b",
      "3aa3fba3d84f49509c8cb6ee915ee8c0",
      "6cb9755789d24adb94c652cc94526390",
      "088265e3f59c4909ac3ced3d0b42abf9",
      "790830890616490fb3dc496e73cb4d08",
      "86dc6a72570540209f199bfcb64ca2d1",
      "1b3a4428615e4ab8a4e576df80b6a5b7",
      "b7f76d00279649f9ac44328a15c3dafe",
      "bea930ddaf2c465bb72059854401348b",
      "a6794f5b1adf43ac8a28cbf9ad6d814e",
      "a5650a8a71fd49e19a787b609120efee",
      "3d191799517b4da5ad52c564a88ed7bf",
      "b0bc9b603150467aa55cb7d142d9d631",
      "7c5935ee42cd45ba85525c9aa13e01c4",
      "48997a190d3b4029a5d0953bcd1a60b0",
      "5d82e826204d44018f983dac33baeaf1",
      "c97887860bcc468fadd411b655f9901c",
      "c0c8d50cd6974dab9b96ebbf83cb50f3",
      "eb41f05a2c9c43229830729941ffe3c4",
      "0b29d6a548df4fe191f5923cbe78e0ad",
      "d6a60e6ae18843928f154c20faa5402f",
      "6be212930eb14553965df8376daaa905",
      "06a179848b454bd585939f6321d2ff0e",
      "f5899bc7158f4c1b8205adc6ab999680",
      "5e2079d1cb9f444880be1bc91e0716d1",
      "2fe6a941b71341b79d5e9c27f04419d9",
      "3859ac40f3134f888957b97eebf403c2",
      "8b28e5e2cd6945a5839684bc7a72b352",
      "ea534ac0f78e4085bf9370c311af9b08",
      "42abf9e0c1554e3992ffee45fe7b251f",
      "4721c406e5e64eb89c519445c4185502",
      "3f2469587e3d41389adbbc18be6a71c2",
      "c7c14fbe1e144f3483740f056be7c97b",
      "76c165f673a148b5968f50316fc5ca08",
      "350184a0e5b64f5589787e4ae05c70a0",
      "9992e1a1b1f04d77a53694e812c232b0",
      "475526f808ba47eda07e2f61905e55b5",
      "9b3ec7a5e45d48d9816e98c5c7bffc8a",
      "94d7f0421c73420198f5c11734e930b9",
      "b8e6d666a3a2497aac5bdb9db2f6a6a9",
      "682f0e53fd464e89870dd8cc0e65ba2b",
      "dab3612259c244318c6d644e056df0a9",
      "a84f6512499d4fb993a24cadc5da3d69",
      "d1257ceb343349b5866b1c1c7cbdd817",
      "7bffe39944984b5f9944fb8d6884ac15",
      "c45c9ce68e464837b7c7766c37534b68",
      "b0f6c1526f784482930a658d4c78047c",
      "9b10bf9eee36428aa3444e084a71bfcc",
      "9e62337ddbe744aca1320e13f206c4f9",
      "206ff72bb1e24f4a91e13e913b58740d",
      "0d76ceab65774cef8c2d5c68dd478d78",
      "afb68ef65d2b43ddb8462da3055a4bf6",
      "7559725bb5734dbfa21fafdb0657afb0",
      "415951a29b204ff7ade3589cfb0b3b6d",
      "b459e13bae2746dd830c6ba5bac81111",
      "038e8fed8fa94332b20389b73197cdb1",
      "e4ac9c48f73a458bab9d2a86630e53d0",
      "24167aa0276441c1a5fcd8c1ce0a47a1",
      "e817ca345d5a40fab75750330dea7955",
      "43e4752a61264b64b568b7fbf295e6cc",
      "f5642ef60a5a4d0ab8cf419cfc035f5d",
      "a8651cd996ea45339845e0032f1f47c1",
      "96823fdfb7994d078018df5422d7ec82",
      "7eb1a33afb44494b8455e0059f8a2212",
      "e7637177d3454b938251ef885ef724bd",
      "9a7feca3079442309cfdccdbe9994f01",
      "26b304a10bbb4ecfbcde85f104fae9b5",
      "a292ec85011e4f69843501a1bb15ad20",
      "aaa2084f23d442118ae1e9abc8471d74",
      "50987c9d6ff042da9e19a589eeb2892b",
      "42ef67fea2124c05a16a8259cf97666b",
      "c0ab062d0e46461a97c440b6d2b9839c",
      "7bd3a76dce0842a0a199fb3a16c95b2a",
      "5c5260c6350a4829ae9b0a583c73d0ac",
      "265d3ecba0d9486aac06c1e6883bcec6",
      "91039b22b32b48f281a392c2a468cdd9",
      "ce3d7957365b4c41a2fa162a574b22f0",
      "df0f722c30ee4cb6b10d238b186e7963",
      "b021d6d3cbc847d2b299608c54396ccc",
      "04a4a0e4ed2649b185fb4bdd6abed404",
      "10aba8d77f20469d993320be98973978",
      "3a72d9b834cb4616818dbfff6bd6f905",
      "168584afa5cb4cc0905b423195972e3d"
     ]
    },
    "id": "0qVO9ZItYbHq",
    "outputId": "f7334b78-bc20-4ccb-80ee-4d24028eca94"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTYf_JuCYbHr"
   },
   "outputs": [],
   "source": [
    "def load_data(filepath, limit=None):\n",
    "    data = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    if limit:\n",
    "        data = data[:limit]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_CHARS = set(\n",
    "    \"абвгґдеєжзиіїйклмнопрстуфхцчшщьюя\"\n",
    "    \"АБВГҐДЕЄЖЗИІЇЙКЛМНОПРСТУФХЦЧШЩЬЮЯ\"\n",
    "    \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    \"0123456789\"\n",
    "    \" \\t\\n\"\n",
    "    \".,;:!?-—–()[]{}\\\"'«»…\"\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = ''.join(c if c in ALLOWED_CHARS else ' ' for c in text)\n",
    "    cleaned = re.sub(r' +', ' ', cleaned)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_chunk(text, min_length=50):\n",
    "    cleaned = clean_text(text)\n",
    "    if len(cleaned) < min_length:\n",
    "        return False\n",
    "\n",
    "    alpha_chars = [c for c in cleaned if c.isalpha()]\n",
    "    if len(alpha_chars) < 20:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(data_folder=DATA_FOLDER, output_path=INDEX_PATH, max_chars=None, use_cache=True):\n",
    "    \"\"\"\n",
    "    Build FAISS index from text files.\n",
    "    \n",
    "    Args:\n",
    "        max_chars: Limit characters per file (e.g., 1_000_000 for ~1MB). None = full file.\n",
    "        use_cache: Load/save chunks from cache to skip Steps 1-2 on reruns.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Try to load cached chunks\n",
    "    if use_cache and os.path.exists(CHUNKS_CACHE) and max_chars is None:\n",
    "        print(f\"Loading cached chunks from {CHUNKS_CACHE}...\")\n",
    "        start = time.time()\n",
    "        with open(CHUNKS_CACHE, \"rb\") as f:\n",
    "            documents = pickle.load(f)\n",
    "        print(f\"  Loaded {len(documents):,} chunks in {time.time()-start:.1f}s\")\n",
    "    else:\n",
    "        txt_files = glob.glob(f\"{data_folder}/*.txt\")\n",
    "        if not txt_files:\n",
    "            raise ValueError(f\"No text files found in {data_folder}\")\n",
    "\n",
    "        print(f\"Found {len(txt_files)} text file(s)\")\n",
    "        for f in txt_files:\n",
    "            size_mb = os.path.getsize(f) / (1024 * 1024)\n",
    "            print(f\"  - {f}: {size_mb:.1f} MB\")\n",
    "        \n",
    "        if max_chars:\n",
    "            print(f\"\\n⚠️  TEST MODE: limiting to {max_chars:,} chars per file\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        # Step 1: Read and chunk files\n",
    "        print(\"\\n[Step 1/3] Reading and chunking files...\")\n",
    "        all_chunks = []\n",
    "        for path in txt_files:\n",
    "            start = time.time()\n",
    "            print(f\"  Reading {os.path.basename(path)}...\", end=\" \", flush=True)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read(max_chars) if max_chars else f.read()\n",
    "            print(f\"{len(text):,} chars\", end=\" -> \", flush=True)\n",
    "            \n",
    "            chunks = text_splitter.create_documents([text], metadatas=[{\"source\": path}])\n",
    "            print(f\"{len(chunks):,} chunks ({time.time()-start:.1f}s)\")\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"  Total raw chunks: {len(all_chunks):,}\")\n",
    "\n",
    "        # Step 2: Filter chunks\n",
    "        print(\"\\n[Step 2/3] Filtering chunks...\")\n",
    "        documents = []\n",
    "        for c in tqdm(all_chunks, desc=\"Filtering\"):\n",
    "            if is_valid_chunk(c.page_content):\n",
    "                c.page_content = clean_text(c.page_content)\n",
    "                documents.append(c)\n",
    "\n",
    "        print(f\"  Valid chunks after filtering: {len(documents):,} ({len(documents)/len(all_chunks)*100:.1f}%)\")\n",
    "        \n",
    "        # Cache chunks for future runs\n",
    "        if use_cache and max_chars is None:\n",
    "            print(f\"\\n  Saving chunks to {CHUNKS_CACHE}...\")\n",
    "            with open(CHUNKS_CACHE, \"wb\") as f:\n",
    "                pickle.dump(documents, f)\n",
    "            print(f\"  Cache saved!\")\n",
    "\n",
    "    # Step 3: Build embeddings and index\n",
    "    print(\"\\n[Step 3/3] Building embeddings and FAISS index...\")\n",
    "    print(f\"  This will process {len(documents):,} chunks...\")\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 256},\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    print(f\"  Embeddings + index built in {time.time()-start:.1f}s\")\n",
    "    \n",
    "    vectorstore.save_local(output_path)\n",
    "    print(f\"\\nIndex saved to {output_path}: {len(documents):,} chunks\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ1ltNfK1BZ-"
   },
   "outputs": [],
   "source": [
    "def load_index(index_path=INDEX_PATH):\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBm4gV3l1Crl"
   },
   "outputs": [],
   "source": [
    "def get_context(vectorstore, question, k=TOP_K):\n",
    "    docs = vectorstore.similarity_search(f\"query: {question}\", k=k)\n",
    "    return \"\\n\".join([f\"[{i+1}] {d.page_content}\" for i, d in enumerate(docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYsITIbXYbHr"
   },
   "outputs": [],
   "source": [
    "def format_prompt(item, tokenizer, context=\"\"):\n",
    "    question = item[\"question\"]\n",
    "    options_text = \"\"\n",
    "    valid_markers = []\n",
    "\n",
    "    for ans in item[\"answers\"]:\n",
    "        options_text += f\"{ans['marker']}) {ans['text']}\\n\"\n",
    "        valid_markers.append(ans[\"marker\"])\n",
    "\n",
    "    system_prompt = \"Ви спеціаліст в українській літературі, мові та історії. Ви знаєте як складаються професійні тести на кшталт ЗНО. Перед вами питання екзамену. Визначьте правильну відповідь. Виведіть ТІЛЬКИ одну букву (А, Б, В, Г або Д) без пояснень і без крапок.\\n\"\n",
    "\n",
    "    if context:\n",
    "        system_prompt = system_prompt + f\"Контекст:\\n{context}\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Питання:{question}\\nВаріанти:\\n{options_text}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text, valid_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vR5BzRksYbHs"
   },
   "outputs": [],
   "source": [
    "def parse_answer(content, valid_markers):\n",
    "    print(content)\n",
    "    match = re.search(r\"([АБВГД])\", content)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"А\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKDZMELpYbHs"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, item, vectorstore=None):\n",
    "    context = get_context(vectorstore, item[\"question\"]) if vectorstore else \"\"\n",
    "    prompt, valid_markers = format_prompt(item, tokenizer, context)\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()\n",
    "    return parse_answer(response, valid_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjRI819LYbHs"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, vectorstore=None, filepath=TRAIN_FILE, limit=DEBUG_LIMIT):\n",
    "    data = load_data(filepath, limit)\n",
    "    correct = 0\n",
    "    total = len(data)\n",
    "    predictions = []\n",
    "\n",
    "    for idx, item in enumerate(tqdm(data, desc=\"Evaluating\")):\n",
    "        predicted = predict(model, tokenizer, item, vectorstore)\n",
    "        expected = item[\"correct_answers\"][0]\n",
    "        is_correct = predicted == expected\n",
    "        predictions.append({\"id\": idx, \"predicted\": predicted, \"expected\": expected, \"correct\": is_correct})\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        tqdm.write(f\"[{idx}] predicted: {predicted}, expected: {expected}, correct: {is_correct}\")\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "    return {\"correct\": correct, \"total\": total, \"accuracy\": accuracy, \"predictions\": pd.DataFrame(predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veaXUulhYbHt"
   },
   "outputs": [],
   "source": [
    "def create_submission(model, tokenizer, vectorstore=None, filepath=TEST_FILE, output=OUTPUT_FILE, limit=DEBUG_LIMIT):\n",
    "    data = load_data(filepath, limit)\n",
    "    results = []\n",
    "\n",
    "    for idx, item in enumerate(tqdm(data, desc=\"Creating submission\")):\n",
    "        predicted = predict(model, tokenizer, item, vectorstore)\n",
    "        results.append({\"id\": item.get(\"id\", idx), \"correct_answers\": predicted})\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output, index=False)\n",
    "    print(f\"Submission saved to {output}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryLlajp7YbHt"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(model, tokenizer, vectorstore=None):\n",
    "    eval_results = evaluate(model, tokenizer, vectorstore)\n",
    "    # df = create_submission(model, tokenizer, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372,
     "referenced_widgets": [
      "73e498f4c1774ad58968df80dbab4085",
      "0a2dcb35b4f64faa82c6e8589fb3e49a",
      "3cb674354c3340048905267a5ab738b5",
      "4519e86b25df4934b44b6d6de491270a",
      "147241da517449d3af8d42e56f52a033",
      "1c6d80e28b4d4080bc2e3be36685922b",
      "71f618aee0aa416ca11665734748494f",
      "542ec1d01b2744d8bfda71fcd8658bfd",
      "b1bc7632202d4520b7b0d602254b2e6a",
      "2fa42adef99b4cae86d6b000a2cef861",
      "46eb7f958aff4237af4c13fa3e054f8c",
      "b118a4218a88417aa6e08040049be32e",
      "39801427303c401f9c6d8ea6054c89b5",
      "d48e30a4744a42c89c3ecda6642b0d6c",
      "b953c78837654552801774ea6f89606a",
      "108bedeeb31342f49101e7f5c8503133",
      "d905bc8230ae4259afb3a34086b26685",
      "a70ed91ed44c4a0b8053306ca095155c",
      "6fda09d0d7554a3990bccc8bcf1bf90b",
      "55668a809be44e2589f05e886c424bc2",
      "7e28386988524a5993dd9bfb3a425ad9",
      "9646845ecc0f4d06990aed15adf5aa4e",
      "dab12fe9f80d40e486dc3d102a74a9e7",
      "87e289b001d242c28fafbd2512320bcc",
      "7bc878d11b4843f38c77a96f2a256b37",
      "e32c6f2f0dd14079873415c93a21e25d",
      "bfaf13ff59c547029ab402a1dabc3184",
      "8fce7229aa804e438b39623df6c9ee6c",
      "d1eecbbcc63747d693a9010d4db74dbb",
      "5a699af5cda6473bb13ce5d6e31c7291",
      "7680ff6df76443bca2ea635675315f47",
      "71137ce283f140a9b284882de7687a76",
      "78362913eb2443f98fca48d512553365",
      "7f8e7dedb6904154994379830e0eeef6",
      "f9e8e1c54db148368386e1f3a219446c",
      "8f865148418e458aae3222b15b53bb1c",
      "2670ab9c119d42829d8d30921c143d15",
      "5c481e69e5114e8ca2ef19468de60038",
      "e25ee80671794c648fd848c8df3b4158",
      "9fe28d3435cb45ce83042ce7f4eb066e",
      "ab51e8c2c87443d48cee3fd732dc1930",
      "e0668f1c46b945c5b19aa7c707935aa7",
      "ee4d6bd49434400aa62dee219604ef75",
      "6f9aa98e4a234ab9b559c03d42c04b69",
      "b41224c0e32c4d3288b75bd2c9261bc6",
      "b0555aa8f4e24bd682ae45d3d7d7120e",
      "2eacd742a0e546029710a828a2e19b8d",
      "08dccb9ba3e54db9b86c8265259e9622",
      "9449b624f8ef4fc8ace8fee7f256a674",
      "d494688321c94fe0bfa120da911d015e",
      "551b40be44b241829ad7fbe31d658e27",
      "3665fe4ddb4b45768848b549aa1fab38",
      "36b8172b5c024395bd4906bb9050678f",
      "4464ca9502664f9d85cbbb6bf2489eb3",
      "602a409b67294d92b2bd46d7ad1e94e0",
      "e428729dd6ba47aab6c1e2b03c298b8c",
      "4379a3d3856b40378bb2c9c338239c1f",
      "adaea6cfc35946e8ada50b97fa1e9be7",
      "45e49d63afe6428b879ff42db447feaf",
      "07c17fe203d94b6aaf8a5e89dfce206a",
      "415595683e344cffb1fb2ce844486c6b",
      "ee767dd605684822a3e8a7d721eb5416",
      "b5508b5784e044569a10bf42b5be275a",
      "77f0b07014bb4404a73cccb87ad38c4a",
      "51e1834adc9a418bbbaa73e9515c0a8d",
      "afe7755794fd4b0c8c1a2e0a1b54ce79",
      "8aeb1f9c69ff4b45a788095663dcb485",
      "b7ab142eefcf4cc588de45171aa14aae",
      "d3cf24757a8242a48dbe808abc789a5a",
      "1fdc7a0363274dd09751e8da55676ebd",
      "c568b34ceaac4047a1b9216934de9543",
      "4bdc6f5273bc429fb1d7791fa423752a",
      "0499ac43d4104e4eb30e32502d9ab54f",
      "9f48f1ec96414e30a6e8d62e89a0489d",
      "670f8040ee194a008fa590337631caef",
      "fd508c1fbd6149228ab99ce772eb269f",
      "c5c0ff655b884e48bc625627a12fa788",
      "35421ede3a0f4eadb2be7eb6fe3225ba",
      "6a043a07c3ac4a04b3125381f05c1494",
      "f8208fec4d1e4a738cb151b19e28331b",
      "1f47e69e0c0945eeb4fcb4d75eba7750",
      "b3b9d4d6c3fe453c80efbc0c96e722c7",
      "54f55aeffe6f418d8c218ebbc75bf95e",
      "df8fab546d08469cbff0bbb8c9db1e2b",
      "4a60619b1efb453bbd46d42909b26dac",
      "755e5e09b1db4069b6036b6c49340400",
      "9cc6a140ebd24d9691df6f706b5698cb",
      "7284931dca88477c87d0c4632f7696ed",
      "ab9b6ca97eda4239ba5bf7a0ee28151e",
      "b30ba39a91194073adc16a4918a534b6",
      "c67dc1a0de9741f988fca2087f112298",
      "cdaa1fbdce334fc09ddfa9f1cf427c62",
      "ab81659816954cfab2c9363a666242a5",
      "8542eed61e56469c8afa51fdacf09beb",
      "2e3116d7efe445c091604a11d288fa94",
      "d6958994190445eabaebdc0424135aee",
      "225946d25e7346b19352dbd9f27e5ab0",
      "1e0daae3cf244a5ca2af4166723ecf88",
      "23b609bbaba14f8cb561b2ca54619458",
      "f77af169e1664b2fa15b0805bb6a3252",
      "3647e9491bd24612b79f05fcee79ac79",
      "2f6cb54ecaf349179fc5f1485e7e1d10",
      "db1bd85b90a445e787d5a1a428f9f2e8",
      "151cac479e604e219a4c04f2bbbccff6",
      "73871d6afe2d413b921b5f75438ca52d",
      "8efc0f3772c64683868fee48c235bd0c",
      "67ac082b11944ab384220cc5002d9f4c",
      "da3eeab5a1994077a70530287b9f4d98",
      "13d4e7518bdc4129a5cf56610b47735b",
      "149f9c4e7ed4408a8063d6bcdf2fc079"
     ]
    },
    "id": "5uyHgnMN3R9X",
    "outputId": "e731a523-8698-4763-9dfb-0419006aae24"
   },
   "outputs": [],
   "source": [
    "# Full run (will cache chunks on first run, then load from cache)\n",
    "vectorstore = build_index()\n",
    "\n",
    "# For testing with small sample:\n",
    "# vectorstore = build_index(max_chars=1_000_000, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PlYItyMYbHt",
    "outputId": "5128ea42-d9a8-400b-a11a-31b2c47af46a"
   },
   "outputs": [],
   "source": [
    "run_pipeline(model, tokenizer, vectorstore)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}